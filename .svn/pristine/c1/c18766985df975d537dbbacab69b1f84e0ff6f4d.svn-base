package com.wisdom.spark.streaming.test

import com.wisdom.spark.common.util.SparkContextUtil
import org.apache.log4j.Logger
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by wisdom on 2017/6/12.
  */
object Hivetest {

  val logger = Logger.getLogger(this.getClass)

  def main(args: Array[String]): Unit = {
    val sc = SparkContextUtil.getInstance()
    //    新建HiveContext对象
    val hiveCtx = new HiveContext(sc)
    //UpdateHiveInput(hiveCtx)
    val lastoccurtime = "2017-06-09 12:00:00"
    val courencetime ="2017-06-09 12:00:00"
    println("select concat(substring(opm_min_collection_timestamp,12,4),(case when substring(opm_min_collection_timestamp,16,1) < 5 then '0' else '5' end)) as alarmTime," +
      " opm_db_host_name,appls_in_db2 from hist_opm.opm_db where length(trim(appls_in_db2)) > 0 and opm_min_collection_timestamp < \' " + courencetime + " \' and opm_min_collection_timestamp >  \'" + lastoccurtime + " \' ")
  }

  def UpdateHiveInput(hiveContext: HiveContext) : Unit = {
    val table = hiveContext.sql("select * from hist_opm.opm_db limit 10")
    //val list = table.map(row => println(row))
    val len = table.collect().toList
    if (len.size == 0 ) { logger.warn("没有连上hive")}
    else len.map(row => println(row))
  }

}
